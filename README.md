# 🧠 Neural Network & Deep Learning Fundamentals

Welcome to my **Neural Network** learning repository! This project represents my journey in understanding the core concepts of neural networks and deep learning using Python and TensorFlow/Keras. From basic perceptrons to full-fledged artificial neural networks (ANNs), this repository contains both theory and practical code implementations.

---

## 📚 What I Learned

### 🔹 Neural Network Basics
- What are neurons and how they work (biological inspiration)
- Understanding perceptrons and multilayer perceptrons (MLPs)
- Activation functions: ReLU, Sigmoid, Tanh
- Loss functions and optimizers
- Forward and backward propagation
- Epochs, batches, and gradient descent

### 🔹 Artificial Neural Networks (ANNs)
- Structure of ANN (input, hidden, output layers)
- Building ANNs using TensorFlow/Keras
- Use of dense layers, compilation, and model fitting
- Evaluation using accuracy and loss metrics
- Handling underfitting and overfitting
- Model saving/loading for reuse
- **Training and testing on the MNIST dataset** for handwritten digit classification

### 🔹 Deep Learning Concepts
- Difference between shallow and deep networks
- Importance of layer depth and parameter tuning
- Role of backpropagation in learning
- Introduction to TensorFlow/Keras APIs

### 🔹 TensorBoard Integration
- Implemented **TensorBoard** to visualize:
  - Model training and validation loss/accuracy curves
  - Histograms of layer weights and biases
  - Graph structure of neural networks
- Learned how to launch and interpret TensorBoard to monitor performance and debugging

---

## 🧪 Sample Results
Some results of my ANN models on dummy and real datasets:
- Accuracy > 95% on **MNIST** handwritten digit classification
- Clear understanding of how changing activation/loss functions affects performance
- Learned how model complexity influences training dynamics

---

## 💡 Key Takeaways
- A solid foundation of how neural networks operate internally
- Confidence in building and training basic ANN models from scratch
- Ability to debug and improve models using hyperparameter tuning
- Practical experience with TensorBoard for model tracking
- Readiness to explore advanced topics like CNNs, RNNs, and Transfer Learning

---

## 🚀 What's Next?
- Dive deeper into **Convolutional Neural Networks (CNNs)** for image processing
- Explore **Recurrent Neural Networks (RNNs)** for sequence modeling
- Learn about **transfer learning** and **pretrained models** (e.g., ResNet, BERT)


---

## 📬 Connect with Me
- **GitHub**: [siddhivdash](https://github.com/siddhivdash)
- **LinkedIn**: [Siddhi Vinayak Dash][(https://www.linkedin.com/in/siddhivdash)](https://www.linkedin.com/in/siddhi-v-dash-3b6a01193/)

---

> *“The best way to learn deep learning is by building neural networks from scratch — and that’s what I’ve done here!”*
